{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING\n",
    "**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "Write requirements to file, anytime you run it, in case you have to go back and recover Kaggle dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n",
    "\n",
    "Latest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download IMDB Movie Review Dataset\n",
    "Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /opt/conda/lib/libuuid.so.1: no version information available (required by wget)\r\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "## Read-in the reviews and print some basic descriptions of them\n",
    "\n",
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization, Stop-word and Punctuation Removal Functions\n",
    "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = row.split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use regular expressions to remove unnecessary characters** \n",
    "\n",
    "Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower() # make all characters lower case\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop-word removal**\n",
    "\n",
    "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "# print(stopwords) # see default stopwords\n",
    "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
    "# of a sentence\n",
    "# stopwords.remove(\"no\")\n",
    "# stopwords.remove(\"nor\")\n",
    "# stopwords.remove(\"not\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model \n",
    "\n",
    "For the computer to make inferences of the e-mails, it has to be able to interpret the text by making a numerical representation of it. One way to do this is by using something called a \"bag-of-words\" model. This model simply counts the frequency of word tokens for each email and thereby represents it as a vector of these counts.\n",
    "\n",
    "** Assemble matrices function**\n",
    "\n",
    "The `assemble_bag()` function assembles a new dataframe containing all the unique words found in the text documents. It counts the word frequency and then returns the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_bag(data):\n",
    "    used_tokens = []\n",
    "    all_tokens = []\n",
    "\n",
    "    for item in data:\n",
    "        for token in item:\n",
    "            if token in all_tokens:\n",
    "                if token not in used_tokens:\n",
    "                    used_tokens.append(token)\n",
    "            else:\n",
    "                all_tokens.append(token)\n",
    "    \n",
    "    df = pd.DataFrame(0, index = np.arange(len(data)), columns = used_tokens)\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        for token in item:\n",
    "            if token in used_tokens:\n",
    "                df.iloc[i][token] += 1    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together To Assemble Dataset\n",
    "\n",
    "Now, putting all the preprocessing steps together we assemble our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "# load data in appropriate form\n",
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    \n",
    "    return data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::data_train::\n",
      "[list(['it', 'was', 'supposed', 'last', 'freddy', 'movie', 'and', '', 'yearsyou', 'would', 'think', 'would', 'tried', 'get', 'good', 'movie', 'done', 'but', 'turned', 'giving', 'us', 'worst', 'series', 'and', 'thats', 'saying', 'lot', 'the', 'plot', 'made', 'sense', 'i', 'seriously', 'cant', 'remember', 'it', 'main', 'characters', 'idiots', 'you', 'really', 'wanted', 'dead', 'freddys', 'wisecracks', 'even', 'worse', 'usual', 'the', 'remotely', 'good', 'bit', 'brief', 'and', 'funny', 'cameo', 'johnny', 'depp', 'the', 'first', 'nightmare', 'movie', 'firstbr', 'br', 'also', 'i', 'originally', 'saw', 'theatre', 'last', 'section', 'reaccounting', 'freddys', 'childhood', 'd', 'wellthe', 'd', 'lousyfaded', 'colors', 'image', 'going', 'focus', 'also', 'three', 'flying', 'skulls', 'supposed', 'scary', 'i', 'think', 'opposite', 'reaction', 'audience', 'everybody', 'broke', 'laughing', 'looks', 'even', 'worse', 'tv', 'd', 'pointless', 'stupidvery', 'dull', 'also', 'skip', 'one', 'see', 'freddy', 'vs', 'jason', 'again'])\n",
      " list(['i', 'really', 'wanted', 'like', 'movie', '', 'location', 'shots', 'mostly', 'filmed', 'pittsburgh', 'trailer', 'wonderful', 'photography', 'but', 'even', 'filmed', 'cartoon', 'really', 'badlymade', 'movie', 'the', 'continuity', 'pacing', 'simply', 'awful', 'the', 'best', 'bits', 'movie', 'ending', 'credits', 'almost', 'worth', 'sticking', 'end', 'though', 'oddly', 'pick', 'little', 'last', 'half', 'hour', 'sobr', 'br', 'when', 'best', 'performance', 'movie', 'andy', 'dick', 'know', 'theres', 'got', 'problem'])\n",
      " list(['i', 'saw', 'little', 'belgian', 'gem', 'two', 'days', 'seeing', 'american', 'teen', 'make', 'mistake', 'it', 'adolescence', 'roller', 'coaster', 'ride', 'american', 'european', 'naissance', 'des', 'pieuvres', 'or', 'called', 'us', 'water', 'lilliesis', 'tale', 'young', '', 'year', 'old', 'girl', 'played', 'pauline', 'acquartwho', 'times', 'resembles', 'young', 'scarlett', 'johanssonacts', 'cool', 'withdrawn', 'girl', 'wants', 'school', 'swim', 'team', 'close', 'another', 'attractive', 'girl', 'adele', 'haenel', 'its', 'obvious', 'marie', 'attracted', 'floriane', 'figuring', 'among', 'maries', 'rather', 'plump', 'unattractive', 'friend', 'anne', 'wants', 'boyfriend', 'like', 'girl', 'age', 'along', 'waywe', 'shown', 'usual', 'array', 'teen', 'pastimes', 'broken', 'heartsshop', 'liftingalcohol', 'andor', 'drug', 'usecasual', 'sexetc', 'this', 'quiet', 'little', 'film', 'takes', 'time', 'work', 'way', 'system', 'michael', 'bay', 'fanstake', 'notethe', 'pacing', 'slowso', 'steer', 'clearbut', 'problem', 'this', 'water', 'lillies', 'charmer', 'no', 'rating', 'herebut', 'would', 'pull', 'hard', 'r', 'due', 'languagenudityadult', 'situations'])\n",
      " ...\n",
      " list(['the', 'biggest', 'mystery', 'veronica', 'mars', 'one', 'tackle', 'screenbr', 'br', 'rather', 'mystery', 'perennial', 'ratings', 'disappointment', 'still', 'air', 'this', 'week', 'marked', 'nadir', 'veronica', 'mars', 'ranked', '', '', 'shows', 'big', '', 'soon', 'big', '', 'yes', 'read', 'right', 'veronica', 'mars', 'beaten', 'every', 'show', 'nowdefunct', 'wb', 'every', 'show', 'upn', 'it', 'beat', 'shows', 'fox', 'course', 'shows', 'abc', 'cbs', 'nbcbr', 'br', 'now', 'hip', 'hypesters', 'going', 'say', 'rerun', 'but', 'everything', 'tv', 'week', 'pretty', 'much', 'rerun', 'it', 'boggles', 'mind', 'cw', 'would', 'choose', 'proved', 'ratings', 'disappointment', 'one', 'shows', 'saved', 'upnbr', 'br', 'clearly', 'something', 'going', 'behind', 'scenes', 'favors', 'exchanged', 'influence', 'peddledbr', 'br', 'sorry', 'cynical', 'explanation', 'there', 'the', 'veronica', 'mars', 'potential', 'line', 'clearly', 'dead', 'two', 'years', 'establish', 'failed', 'sobr', 'br', 'maybe', 'joel', 'silvers', 'influence'])\n",
      " list(['germans', 'stand', 'open', 'get', 'mowed', 'machine', 'gun', 'good', 'guys', 'never', 'die', 'unless', 'dramatic', 'purposes', 'plot', 'many', 'holes', 'laughable', 'where', 'german', 'soldiers', 'go', 'rolled', 'fuel', 'tank', 'towards', 'train', 'erik', 'estrada', 'please', 'and', 'whole', 'idea', 'hijacking', 'train', 'how', 'moronic', 'that', 'the', 'germans', 'know', 'going', 'go', 'like', 'leave', 'track', 'drive', 'away', 'what', 'waste', 'i', 'would', 'rather', 'bonk', 'head', 'ball', 'peen', 'hammer', '', 'times', 'sit', 'again', 'i', 'mean', 'seriously', 'felt', 'like', 'made', 's', 'produced', '', '', 'ateam', 'believable', 'horrid', 'excuse', 'movie', 'only', 'watch', 'need', 'good', 'laugh', 'this', 'movie', 'tele', 'sevalas', 'green', 'beret', 'john', 'wayne'])\n",
      " list(['can', 'i', 'simple', 'primitive', 'evaluations', 'simply', 'say', 'i', 'liked', 'it', 'its', 'reasonably', 'funny', 'bits', 'got', 'great', 'stars', 'gorgeous', 'look', 'at', 'the', 'songs', 'there', 'two', 'repeated', 'forgettable', 'get', 'healthy', 'ironic', 'treatment', 'such', 'terribly', 'handsome', 'mr', 'fairbanks', 'exploding', 'frenzied', 'wagnerian', 'version', 'tender', 'ballad', 'miss', 'grable', 'rendered', 'much', 'dancing', '', 'crinolines', 'draped', 'around', 'miss', 'grable', 'comedy', 'might', 'bit', 'heavy', 'handed', 'result', 'still', 'uplifting', 'the', 'photography', 'including', 'real', 'outdoor', 'shots', 'thrill', 'amazing', 'playing', 'around', 'different', 'shades', 'lush', 'heavy', 'gold', 'miss', 'grable', 'bit', 'past', 'prime', 'plumpish', 'side', 'still', 'fresh', 'comfortable', 'continental', 'olde', 'worlde', 'comedy', 'its', 'pretty', 'much', 'along', 'line', 'down', 'earth', 'rita', 'hayworth', 'one', 'tends', 'rather', 'disliked', 'many', 'so', 'i', 'suppose', 'several', 'people', 'would', 'deem', 'that', 'lady', 'ermine', 'outdated', 'stuffy', 'but', 'fairy', 'tale', 'tend', 'move', 'along', 'certain', 'paste'])]\n"
     ]
    }
   ],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]\n",
    "\n",
    "print(\"DEBUG::data_train::\")\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1020  980]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>would</th>\n",
       "      <th>movie</th>\n",
       "      <th>and</th>\n",
       "      <th>it</th>\n",
       "      <th>the</th>\n",
       "      <th>good</th>\n",
       "      <th>i</th>\n",
       "      <th>last</th>\n",
       "      <th>freddys</th>\n",
       "      <th>d</th>\n",
       "      <th>...</th>\n",
       "      <th>housebr</th>\n",
       "      <th>spawn</th>\n",
       "      <th>hulk</th>\n",
       "      <th>perennial</th>\n",
       "      <th>marked</th>\n",
       "      <th>nadir</th>\n",
       "      <th>veronica</th>\n",
       "      <th>exchanged</th>\n",
       "      <th>grable</th>\n",
       "      <th>hayworth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 11592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      would  movie  and  it  the  good  i  last  freddys  d  ...  housebr  \\\n",
       "0         2      3    3   2    3     2  3     2        2  3  ...        0   \n",
       "1         0      4    0   0    2     0  1     1        0  0  ...        0   \n",
       "2         1      0    0   1    0     0  1     0        0  0  ...        0   \n",
       "3         0      0    0   4    3     0  0     0        0  0  ...        0   \n",
       "4         0      1    0   1    0     0  0     0        0  0  ...        0   \n",
       "...     ...    ...  ...  ..  ...   ... ..   ...      ... ..  ...      ...   \n",
       "1995      2      0    1   1    0     0  9     0        0  0  ...        0   \n",
       "1996      0      7    0   0    1     1  1     0        0  0  ...        1   \n",
       "1997      1      0    0   2    2     0  0     0        0  0  ...        0   \n",
       "1998      1      2    1   0    1     2  2     0        0  0  ...        0   \n",
       "1999      1      0    0   1    2     0  3     0        0  0  ...        0   \n",
       "\n",
       "      spawn  hulk  perennial  marked  nadir  veronica  exchanged  grable  \\\n",
       "0         0     0          0       0      0         0          0       0   \n",
       "1         0     0          0       0      0         0          0       0   \n",
       "2         0     0          0       0      0         0          0       0   \n",
       "3         0     0          0       0      0         0          0       0   \n",
       "4         0     0          0       0      0         0          0       0   \n",
       "...     ...   ...        ...     ...    ...       ...        ...     ...   \n",
       "1995      0     0          0       0      0         0          0       0   \n",
       "1996      1     1          0       0      0         0          0       0   \n",
       "1997      0     0          1       1      1         4          1       0   \n",
       "1998      0     0          0       0      0         0          0       0   \n",
       "1999      0     0          0       0      0         0          0       3   \n",
       "\n",
       "      hayworth  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "1995         0  \n",
       "1996         0  \n",
       "1997         0  \n",
       "1998         0  \n",
       "1999         1  \n",
       "\n",
       "[2000 rows x 11592 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MixedBagOfReviews = assemble_bag(data_train)\n",
    "# this is the list of words in our bag-of-words model\n",
    "predictors = [column for column in MixedBagOfReviews.columns]\n",
    "\n",
    "# expand default pandas display options to make emails more clearly visible when printed\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "MixedBagOfReviews # you could do print(MixedBagOfReviews), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x/train_y list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[2 3 3 ... 0 0 0]\n",
      " [0 4 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [2 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0 0 1 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# split into independent 70% training and 30% testing sets\n",
    "data = MixedBagOfReviews.values\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = header[:idx]\n",
    "# remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = header[idx:] \n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about other vectorization strategies?\n",
    "\n",
    "We present other vectorization strategies below, for readers who are interested in exploring them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "# create the transform - uncomment the one you want to focus on\n",
    "# vectorizer = CountVectorizer() # this is equivalent to the bag of words\n",
    "vectorizer = TfidfVectorizer() # tf-idf vectorizer\n",
    "# vectorizer = HashingVectorizer(n_features=3000) # hashing vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24595\n",
      "(1, 24595)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "vectorizer.fit([' '.join(sublst) for sublst in data_train])\n",
    "# summarize\n",
    "print(len(vectorizer.vocabulary_))\n",
    "#print(vectorizer.idf_)\n",
    "# encode one document\n",
    "vector = vectorizer.transform([' '.join(data_train[0])])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "\n",
    "USE = False # set this to 'True' if you want to use the vectorizer featurizers instead of the bag-of-words done before\n",
    "if(USE):\n",
    "    data = vectorizer.transform([' '.join(sublst) for sublst in data_train]).toarray()\n",
    "    # 70% of data for training\n",
    "    train_x = data[:idx,:]\n",
    "    # remaining 30% for testing\n",
    "    test_x = data[idx:,:]\n",
    "\n",
    "    print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "    print(train_x.shape[0])\n",
    "    print(train_x)\n",
    "    print(train_y[:5])\n",
    "    print(len(train_y))\n",
    "    predictors = [column for column in vectorizer.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit(train_x,train_y):\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    try:\n",
    "        model.fit(train_x, train_y)\n",
    "    except:\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "model = fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::The logistic regression predicted labels are::\n",
      "[0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
      " 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1\n",
      " 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0\n",
      " 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0\n",
      " 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1\n",
      " 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict(test_x)\n",
    "\n",
    "# print all labels for full trasparency\n",
    "print(\"DEBUG::The logistic regression predicted labels are::\")\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression accuracy score is::\n",
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(test_y, predicted_labels)\n",
    "\n",
    "print(\"The logistic regression accuracy score is::\")\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import SVC # Support Vector Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVC Classifier took  33 seconds\n",
      "DEBUG::The SVC Classifier predicted labels are::\n",
      "[0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
      " 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 1\n",
      " 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0\n",
      " 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 1]\n",
      "The SVC Classifier testing accuracy score is::\n",
      "0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "# Create a support vector classifier\n",
    "clf = SVC(C=1, gamma=\"auto\", kernel='linear',probability=False)\n",
    "\n",
    "# Fit the classifier using the training data\n",
    "start_time = time.time()\n",
    "clf.fit(train_x, train_y)\n",
    "end_time = time.time()\n",
    "print(\"Training the SVC Classifier took %3d seconds\"%(end_time-start_time))\n",
    "\n",
    "# test and evaluate\n",
    "predicted_labels = clf.predict(test_x)\n",
    "print(\"DEBUG::The SVC Classifier predicted labels are::\")\n",
    "print(predicted_labels)\n",
    "\n",
    "acc_score = accuracy_score(test_y, predicted_labels)\n",
    "print(\"The SVC Classifier testing accuracy score is::\")\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest Classifier took   0 seconds\n",
      "DEBUG::The RF predicted labels are::\n",
      "[0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
      " 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
      " 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
      " 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0\n",
      " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
      " 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
      " 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1\n",
      " 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 0]\n",
      "DEBUG::The RF testing accuracy score is::\n",
      "0.7116666666666667\n"
     ]
    }
   ],
   "source": [
    "# Load scikit's random forest classifier library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf = RandomForestClassifier(n_jobs=1, random_state=0)\n",
    "\n",
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y (spam, not spam?)\n",
    "start_time = time.time()\n",
    "clf.fit(train_x, train_y)\n",
    "end_time = time.time()\n",
    "print(\"Training the Random Forest Classifier took %3d seconds\"%(end_time-start_time))\n",
    "\n",
    "predicted_labels = clf.predict(test_x)\n",
    "print(\"DEBUG::The RF predicted labels are::\")\n",
    "print(predicted_labels)\n",
    "\n",
    "acc_score = accuracy_score(test_y, predicted_labels)\n",
    "\n",
    "print(\"DEBUG::The RF testing accuracy score is::\")\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9136\n",
      "AUC Score (Train): 0.981752\n",
      "CV Score : Mean - 0.845347 | Std - 0.01020305 | Min - 0.836148 | Max - 0.8625899\n",
      "Training the Gradient Boosting Classifier took 542 seconds\n",
      "DEBUG::The Gradient Boosting predicted labels are::\n",
      "[0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
      " 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
      " 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1\n",
      " 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
      " 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
      " 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
      " 0 0 1 0 0 0 0 1]\n",
      "DEBUG::The Gradient Boosting testing accuracy score is::\n",
      "0.76\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  # GBM algorithm\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "def modelfit(alg, train_x, train_y, predictors, test_x, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_x, train_y)\n",
    "        \n",
    "    #Predict training set:\n",
    "    predictions = alg.predict(train_x)\n",
    "    predprob = alg.predict_proba(train_x)[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, train_x, train_y, cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_y,predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_y, predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if printFeatureImportance:\n",
    "        fig,ax = plt.subplots()\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp[:10].plot(kind='bar', title='Feature Importances',ax=ax)\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "        fig.savefig('GBMimportances.eps', format='eps',bbox_inches='tight')\n",
    "        fig.savefig('GBMimportances.pdf', format='pdf',bbox_inches='tight')\n",
    "        fig.savefig('GBMimportances.png', format='png',bbox_inches='tight')\n",
    "        fig.savefig('GBMimportances.svg', format='svg',bbox_inches='tight')\n",
    "        \n",
    "    return alg.predict(test_x)\n",
    "        \n",
    "gbm = GradientBoostingClassifier(random_state=10)\n",
    "\n",
    "start_time = time.time()\n",
    "test_predictions = modelfit(gbm, train_x, train_y, predictors, test_x)\n",
    "end_time = time.time()\n",
    "print(\"Training the Gradient Boosting Classifier took %3d seconds\"%(end_time-start_time))\n",
    "\n",
    "predicted_labels = test_predictions\n",
    "print(\"DEBUG::The Gradient Boosting predicted labels are::\")\n",
    "print(predicted_labels)\n",
    "\n",
    "acc_score = accuracy_score(test_y, predicted_labels)\n",
    "\n",
    "print(\"DEBUG::The Gradient Boosting testing accuracy score is::\")\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make figures downloadable to local system in interactive mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=GBMimportances.svg>Download file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link(filename='GBMimportances.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must remove all downloaded files - having too many of them on completion will make Kaggle reject your notebook \n",
    "!rm -rf aclImdb\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
