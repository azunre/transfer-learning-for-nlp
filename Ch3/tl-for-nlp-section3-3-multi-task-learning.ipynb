{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/epfml/sent2vec\r\n",
      "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-dcne7kr1\r\n",
      "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-dcne7kr1\r\n",
      "Building wheels for collected packages: sent2vec\r\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1137369 sha256=e731ace1b828f01b8ff79fc319495c980451af429c14c91ee788f30c091afcab\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0nw9k9lq/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\r\n",
      "Successfully built sent2vec\r\n",
      "Installing collected packages: sent2vec\r\n",
      "Successfully installed sent2vec-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "# install sent2vec\n",
    "!pip install git+https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n",
    "\n",
    "Latest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download IMDB Movie Review Dataset\n",
    "Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "## Read-in the reviews and print some basic descriptions of them\n",
    "\n",
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenization, Stop-word and Punctuation Removal Functions\n",
    "Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\n",
    "maxtokens = 200 # the maximum number of tokens per document\n",
    "maxtokenlen = 100 # the maximum length of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row is None or row is '':\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use regular expressions to remove unnecessary characters**\n",
    "\n",
    "Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def reg_expressions(row):\n",
    "    tokens = []\n",
    "    try:\n",
    "        for token in row:\n",
    "            token = token.lower() # make all characters lower case\n",
    "            token = re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token = token[:maxtokenlen] # truncate token\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token = \"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop-word removal**\n",
    "\n",
    "Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')    \n",
    "\n",
    "# print(stopwords) # see default stopwords\n",
    "# it may be beneficial to drop negation words from the removal list, as they can change the positive/negative meaning\n",
    "# of a sentence\n",
    "# stopwords.remove(\"no\")\n",
    "# stopwords.remove(\"nor\")\n",
    "# stopwords.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Embedding Vectors\n",
    "The following functions are used to extract sent2vec embedding vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sent2vec embedding took 7 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sent2vec\n",
    "\n",
    "s2v_model = sent2vec.Sent2vecModel()\n",
    "start=time.time()\n",
    "s2v_model.load_model('../input/sent2vec/wiki_unigrams.bin')\n",
    "end = time.time()\n",
    "print(\"Loading the sent2vec embedding took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_embedding_vectors(data):\n",
    "    out = None\n",
    "    for item in data:\n",
    "        vec = s2v_model.embed_sentence(\" \".join(item))\n",
    "        if vec is not None:\n",
    "            if out is not None:\n",
    "                out = np.concatenate((out,vec),axis=0)\n",
    "            else:\n",
    "                out = vec                                            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together To Assemble Dataset\n",
    "Now, putting all the preprocessing steps together we assemble our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "# load data in appropriate form\n",
    "def load_data(path):\n",
    "    data, sentiments = [], []\n",
    "    for folder, sentiment in (('neg', 0), ('pos', 1)):\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder, name), 'r') as reader:\n",
    "                  text = reader.read()\n",
    "            text = tokenize(text)\n",
    "            text = stop_word_removal(text)\n",
    "            text = reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np = np.array(data)\n",
    "    data, sentiments = unison_shuffle_data(data_np, sentiments)\n",
    "    \n",
    "    return data, sentiments\n",
    "\n",
    "train_path = os.path.join('aclImdb', 'train')\n",
    "test_path = os.path.join('aclImdb', 'test')\n",
    "raw_data, raw_header = load_data(train_path)\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::data_train::\n",
      "[list(['its', 'sad', 'romanian', 'audiences', 'still', 'populated', 'vulgar', 'uneducated', 'individuals', 'relish', 'kind', 'cheap', 'demonstrative', 'shows', 'superficial', 'brutal', 'garcea', 'series', 'vacanta', 'mare', 'childplays', 'the', 'difference', 'mugur', 'mihäescu', 'doru', 'octavian', 'dumitru', 'subartisans', 'never', 'presume', 'claim', 'shows', 'art', 'pintilie', '', 'years', 'ago', 'made', 'good', 'movie', 'duminicä', 'la', 'ora', 'sase', 'followed', 'another', 'one', 'nice', 'enough', 'reconstituirea', 'tries', 'declare', 'filmlenghts', 'art', 'works', '', 'but', 'unfortunately', 'masters', 'way', 'limited', 'level', 'specifically', 'cinematographic', 'means', 'expression', 'as', 'such', 'niki', 'ardelean', 'offers', 'sample', 'how', 'not', '', 'merit'])\n",
      " list(['one', 'previous', 'reviewers', 'wrote', 'appeared', 'middle', 'ground', 'opinions', 'love', 'story', 'one', 'loved', 'hated', 'it', 'but', 'seems', 'remarkable', 'distribution', 'opinions', 'throughout', 'scale', '', '', 'for', 'me', 'movie', 'rated', '', 'there', 'beautiful', 'scenes', 'locations', 'ray', 'milland', 'turns', 'fabulous', 'job', 'olivers', 'father', 'but', 'movie', 'particularly', 'compelling', 'job', 'telling', 'story', 'story', 'unique', 'warrant', 'multiple', 'viewings', 'least', 'me', 'i', 'may', 'bit', 'snob', 'i', 'tend', 'avoid', 'movies', 'ryan', 'oneal', '', 'i', 'still', 'seen', 'barry', 'lyndon', '', 'them', 'all', 'ruined', 'presence', 'the', 'lone', 'exception', 'whats', 'up', 'doc', 'straight', 'performance', 'perfect', 'underlining', 'barbra', 'streisands', 'goofball', 'protagonist', '', 'and', 'coincidentally', 'takes', 'shot', 'love', 'story', 'good', 'measure', 'mcgraw', 'oneal', 'tend', 'mug', 'lines', 'rather', 'act', 'thembr', 'br', 'this', 'movie', 'notable', 'beginning', 'one', 'fine', 'career', 'tommy', 'lee'])\n",
      " list(['for', 'hasbeens', 'never', 'wass', 'curious', 'film', 'youever', 'played', 'sport', 'wondered', 'felt', 'like', 'lights', 'went', 'crowd', 'leftthis', 'film', 'explores', 'morebr', 'br', 'robin', 'williamsjack', 'dundee', 'small', 'town', 'assistant', 'banker', 'taft', 'ca', 'whose', 'life', 'plagued', 'miscue', 'big', 'rival', 'high', 'school', 'football', 'game', '', 'years', 'ago', 'dropped', 'pass', 'would', 'bakersfield', 'archrival', 'takes', 'great', 'pleasure', 'pounding', 'taft', 'rockets', 'season', 'season', '', 'kurt', 'russellreno', 'hightower', 'quarterback', 'famous', 'game', 'local', 'legend', 'van', 'repair', 'specialist', 'whose', 'life', 'fading', 'lethargy', 'like', 'town', 'taft', 'itselfbr', 'br', 'williams', 'gets', 'idea', 'remake', 'history', 'replaying', 'game', '', 'he', 'meets', 'skeptical', 'resistance', 'goes', 'one', 'man', 'terror', 'spree', 'literally', 'paints', 'town', '', 'orange', 'yellow', 'black', '', 'raise', 'ire', 'residents', 'recreate', 'the', 'game', '', 'after', 'succeeding', 'players', '', 'team', 'reunite', 'try', 'get', 'shape', 'practice'])\n",
      " ...\n",
      " list(['vipul', 'shah', 'done', 'really', 'impressive', 'work', 'filmmaker', 'past', 'waqt', '', 'the', 'race', 'against', 'time', 'namaste', 'london', 'entertaining', 'interesting', 'watch', 'singh', 'is', 'kinng', 'fun', 'produced', 'his', 'latest', 'outing', 'filmmaker', 'london', 'dreams', 'comes', 'careers', 'weakest', 'farebr', 'br', 'london', 'dreams', 'mediocre', 'storyline', 'success', 'turns', 'friendship', 'hatred', 'agreed', 'potential', 'watch', 'london', 'dreams', 'wonder', 'whats', 'happening', 'this', 'film', 'maybe', 'worst', 'climax', 'recent', 'times', 'vipul', 'shah', 'writer', 'puts', 'vipul', 'shah', 'director', 'down', 'br', 'br', 'the', 'first', 'hour', 'boring', 'the', 'second', 'hour', 'better', 'climax', 'horrendous', 'how', 'anyone', 'forgive', 'person', 'decided', 'destroy', 'you', 'i', 'wont', 'ajay', 'devgn', 'suddenly', 'decides', 'go', 'india', 'ask', 'forgiveness', 'diaper', 'buddy', 'thanks', 'uncle', 'om', 'puri', 'when', 'reaches', 'india', 'rather', 'slapping', 'abusing', 'salman', 'welcomes', 'band', 'baja', 'says', 'reason', 'behind', 'entire', 'fiasco', 'was', 'vipul', 'shahs', 'intension', 'show', 'salmans', 'character', 'god', 'if', 'yes', 'failed', 'completely', 'the', 'question'])\n",
      " list(['probably', 'new', 'zealands', 'worst', 'movie', 'ever', 'madebr', 'br', 'the', 'jokes', 'they', 'funny', 'used', 'movies', '', 'plain', 'corny', 'the', 'acting', 'is', 'bad', 'even', 'though', 'great', 'castbr', 'br', 'the', 'story', 'uninteresting', '', 'boring', 'has', 'cheese', 'pizza', 'huts', 'cheese', 'lovers', 'pizza', 'kind', 'like', 'acting', 'has', '', 'times', 'beforebr', 'br', 'i', 'watched', 'came', 'tv', 'boring', 'could', 'stand', '', 'minutes', 'it', 'br', 'br', 'this', 'movie', 'sucksbr', 'br', 'do', 'watch', 'it', 'br', 'br', 'watch', 'paint', 'dry', 'instead'])\n",
      " list(['as', 'veteran', 'many', 'many', 'pretentious', 'french', 'films', 'i', 'thought', 'id', 'taken', 'worst', 'industry', 'offer', 'able', 'stomach', 'anything', 'but', 'this', 'pointless', 'relentless', 'violent', 'unpleasant', 'meaningless', '', 'the', 'film', 'nothing', 'offer', 'random', 'hatred', 'aggression', 'dressed', 'pretentious', 'art', 'avoid', 'costs'])]\n"
     ]
    }
   ],
   "source": [
    "# Subsample required number of samples\n",
    "random_indices = np.random.choice(range(len(raw_header)),size=(Nsamp*2,),replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]\n",
    "\n",
    "del raw_data, raw_header # huge and no longer needed, get rid of it\n",
    "\n",
    "print(\"DEBUG::data_train::\")\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments and their frequencies:\n",
      "[0 1]\n",
      "[1018  982]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments and their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02599724 -0.08517347 -0.16715904 ... -0.08310642 -0.14614873\n",
      "  -0.01413972]\n",
      " [-0.04134955 -0.14229763 -0.07826066 ... -0.06103064 -0.09036611\n",
      "  -0.01086542]\n",
      " [-0.2049824  -0.1873562  -0.00921628 ...  0.05132209 -0.07271242\n",
      "   0.10262631]\n",
      " ...\n",
      " [-0.00128172 -0.08218187  0.07703825 ... -0.03221888 -0.10659809\n",
      "   0.10856076]\n",
      " [ 0.32903692  0.02277061  0.17804019 ...  0.0747768   0.14600788\n",
      "   0.05995347]\n",
      " [ 0.02151151 -0.05121614  0.00920535 ... -0.33328614  0.17787023\n",
      "   0.18338902]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(data_train)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x/train_y list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[-2.59972438e-02 -8.51734728e-02 -1.67159036e-01 ... -8.31064209e-02\n",
      "  -1.46148726e-01 -1.41397202e-02]\n",
      " [-4.13495526e-02 -1.42297626e-01 -7.82606602e-02 ... -6.10306412e-02\n",
      "  -9.03661102e-02 -1.08654182e-02]\n",
      " [-2.04982400e-01 -1.87356204e-01 -9.21628159e-03 ...  5.13220876e-02\n",
      "  -7.27124214e-02  1.02626309e-01]\n",
      " ...\n",
      " [-4.10042219e-02 -2.07076460e-01  6.78418064e-03 ... -5.20211458e-02\n",
      "  -3.28047015e-02 -2.64326066e-01]\n",
      " [-3.85371745e-02 -7.63729662e-02 -9.11653042e-05 ...  1.52520820e-01\n",
      "  -1.71623066e-01  1.00249372e-01]\n",
      " [ 1.03981026e-01  8.97879079e-02 -1.11229941e-01 ...  4.81342077e-02\n",
      "  -1.26129508e-01  1.89728320e-01]]\n",
      "[0 0 1 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "data = EmbeddingVectors\n",
    "del EmbeddingVectors\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = header[idx:] \n",
    "\n",
    "print(\"train_x/train_y list details, to make sure it is of the right form:\")\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print(train_y[:5])\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single IMDB Task Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "input_shape = (len(train_x[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 889us/step - loss: 0.5754 - accuracy: 0.7079 - val_loss: 0.4711 - val_accuracy: 0.7983\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 116us/step - loss: 0.4126 - accuracy: 0.8179 - val_loss: 0.4222 - val_accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 0.3858 - accuracy: 0.8264 - val_loss: 0.4148 - val_accuracy: 0.8200\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 145us/step - loss: 0.3690 - accuracy: 0.8243 - val_loss: 0.4090 - val_accuracy: 0.8217\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 0.2983 - accuracy: 0.8793 - val_loss: 0.3736 - val_accuracy: 0.8150\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.2794 - accuracy: 0.8786 - val_loss: 0.4752 - val_accuracy: 0.7967\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 0.2980 - accuracy: 0.8679 - val_loss: 0.3781 - val_accuracy: 0.8217\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 0.2460 - accuracy: 0.9021 - val_loss: 0.3815 - val_accuracy: 0.8217\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.2265 - accuracy: 0.9079 - val_loss: 0.3947 - val_accuracy: 0.8200\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 135us/step - loss: 0.2109 - accuracy: 0.9150 - val_loss: 0.4116 - val_accuracy: 0.8217\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Email Task, Train Single Email Task Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Enron dataset and get a sense for the data by printing sample messages to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns!\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "filepath = \"../input/enron-email-dataset/emails.csv\"\n",
    "\n",
    "# Read the enron data into a pandas.DataFrame called emails\n",
    "emails = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\n",
    "print(emails.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate headers from the message bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "\n",
    "def extract_messages(df):\n",
    "    messages = []\n",
    "    for item in df[\"message\"]:\n",
    "        # Return a message object structure from a string\n",
    "        e = email.message_from_string(item)    \n",
    "        # get message body  \n",
    "        message_body = e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from e-mails!\")\n",
    "    return messages\n",
    "\n",
    "bodies = extract_messages(emails)\n",
    "\n",
    "del emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Vince,\\n\\nI have added a lot of material to \"fill in the wholes\" and would like your\\nreaction to the current draft.  I am still not very happy with the risk\\nmanagement segment (primarily as a result of my own lack of knowledge) so\\nplease read it carefully and get me your comments.\\n\\nI plan t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Portland Web Server will be going down for 20 min at 1:45 instead of \\n12:00 noon.\\n\\nRegards,\\nPaul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I don't understand the netting that you are referring to.  Does it mean that they will pay negative ctc's up to some just and reasonable amount and anything above the j&amp;r level will be paid out when all of the other people are paid.  How do they define the \"ESP's share of the undercollection?\"\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Group,\\n\\nEffective January 31st, Collin will be leaving the real time group and \\ntransferring to IT.  \\nWe wish Collin success in his new endeavors.\\n\\nBill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>John --\\n\\nThanks for the update.  As long as the FERC is fully in control and the ERO is not a \"self-regulating\" organization, I assume we'd be ok.  The ERO must be fully under the control of FERC.\\n\\nJim\\n\\n\\n\\n\\n -----Original Message-----\\nFrom: \\tShelk, John  \\nSent:\\tFriday, September 14, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  Vince,\\n\\nI have added a lot of material to \"fill in the wholes\" and would like your\\nreaction to the current draft.  I am still not very happy with the risk\\nmanagement segment (primarily as a result of my own lack of knowledge) so\\nplease read it carefully and get me your comments.\\n\\nI plan t...\n",
       "1                                                                                                                                                                                                     The Portland Web Server will be going down for 20 min at 1:45 instead of \\n12:00 noon.\\n\\nRegards,\\nPaul\n",
       "2  I don't understand the netting that you are referring to.  Does it mean that they will pay negative ctc's up to some just and reasonable amount and anything above the j&r level will be paid out when all of the other people are paid.  How do they define the \"ESP's share of the undercollection?\"\\n...\n",
       "3                                                                                                                                               Group,\\n\\nEffective January 31st, Collin will be leaving the real time group and \\ntransferring to IT.  \\nWe wish Collin success in his new endeavors.\\n\\nBill\n",
       "4  John --\\n\\nThanks for the update.  As long as the FERC is fully in control and the ERO is not a \"self-regulating\" organization, I assume we'd be ok.  The ERO must be fully under the control of FERC.\\n\\nJim\\n\\n\\n\\n\\n -----Original Message-----\\nFrom: \\tShelk, John  \\nSent:\\tFriday, September 14, ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract random 10000 enron email bodies for building dataset\n",
    "import random\n",
    "bodies_df = pd.DataFrame(random.sample(bodies, 10000))\n",
    "\n",
    "del bodies # these are huge, no longer needed, get rid of them\n",
    "\n",
    "# expand default pandas display options to make emails more clearly visible when printed\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "bodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Preprocess Fraudulent \"419\" Email Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../input/fraudulent-email-corpus/fradulent_emails.txt\"\n",
    "with open(filepath, 'r',encoding=\"latin1\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on the code word `From r` appearing close to the beginning of each email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3978 spam emails!\n"
     ]
    }
   ],
   "source": [
    "fraud_emails = data.split(\"From r\")\n",
    "\n",
    "del data\n",
    "\n",
    "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from e-mails!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                             0\n",
       "0  FROM:MR. JAMES NGOLA.\\nCONFIDENTIAL TEL: 233-27-587908.\\nE-MAIL: (james_ngola2002@maktoob.com).\\n\\nURGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\\n\\n\\nDEAR FRIEND,\\n\\nI AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY G...\n",
       "1  Dear Friend,\\n\\nI am Mr. Ben Suleman a custom officer and work as Assistant controller of the Customs and Excise department Of the Federal Ministry of Internal Affairs stationed at the Murtala Mohammed International Airport, Ikeja, Lagos-Nigeria.\\n\\nAfter the sudden death of the former Head of s...\n",
       "2  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "3  FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF ELEME KINGDOM \\nCHIEF DANIEL ELEME, PHD, EZE 1 OF ELEME.E-MAIL \\nADDRESS:obong_715@epatra.com  \\n\\nATTENTION:PRESIDENT,CEO Sir/ Madam. \\n\\nThis letter might surprise you because we have met\\nneither in person nor by correspondence. But I believe\\nit is...\n",
       "4  Dear sir, \\n \\nIt is with a heart full of hope that I write to seek your help in respect of the context below. I am Mrs. Maryam Abacha the former first lady of the former Military Head of State of Nigeria General Sani Abacha whose sudden death occurred on 8th of June 1998 as a result of cardiac ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"]))\n",
    "\n",
    "del fraud_emails\n",
    "\n",
    "fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n",
    "\n",
    "del fraud_bodies\n",
    "\n",
    "fraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_word_removal)\n",
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "del bodies_df\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_word_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "del fraud_bodies_df\n",
    "\n",
    "raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data is:\n",
      "(2000,)\n",
      "Data is:\n",
      "[list(['emailmessagemessage', 'object', 'xfaeef', 'emailmessagemessage', 'object', 'xfab'])\n",
      " list(['my', 'deare', 'greetings', 'miss', 'fatoumata', 'saleebye', 'i', 'writting', 'letter', 'due', 'respect', 'heartful', 'tears', 'since', 'known', 'met', 'previouslyc', 'i', 'asking', 'love', 'love', 'well', 'i', 'gone', 'profile', 'pick', 'interest', 'youe', 'i', 'fair', 'complexionc', 'i', 'fit', 'in', 'tallc', 'i', 'love', 'sport', 'eventsc', 'going', 'outdoors', 'activitiesc', 'watching', 'movingc', 'shoppingc', 'walking', 'etcein', 'nutshellc', 'my', 'name', 'miss', 'fatoumata', 'saleeby', '', 'years', 'old', 'republic', 'liberia', 'west', 'africac', 'seeking', 'refugee', 'dakarsenegal', 'unhcrei', 'therefore', 'write', 'inform', 'i', 'surviving', 'child', 'daughter', 'deceased', 'dre', 'elie', 'ee', 'saleeby', 'former', 'minister', 'of', 'finance', 'f', 'executive', 'director', 'central', 'bank', 'of', 'liberia', 'year', 'july', 'febe', 'e', 'the', 'reason', 'correspondence', 'formally', 'request', 'mutual', 'assistance', 'business', 'relationship', 'order', 'benefit', 'wealth', 'experience', 'businesse', 'the', 'secret', 'fact', 'thereforec', 'father', 'kept', 'sum', 'uscc', 'e', 'four', 'million', 'seven', 'hundred', 'thousand', 'us', 'dollars', 'cashe', 'the', 'money', 'deposited'])\n",
      " list(['i', 'humbly', 'introduce', 'you', 'i', 'mrs', 'alhaja', 'rashidat', 'azmil', 'wife', 'ofcaptain', 'azmil', 'served', 'iniraqi', 'contacting', 'base', 'trust', 'humanitarian', 'assistance', 'iurgently', 'need', 'fromyou', 'the', 'recent', 'concluded', 'us', 'led', 'war', 'country', 'left', 'familytotally', 'devastated', 'ilost', 'husband', 'cold', 'hands', 'death', 'children', 'lost', 'theifatherlong', 'warstarted', 'late', 'husband', 'custody', '', 'million', 'us', 'dollars', 'dueto', 'serious', 'threat', 'warfrom', 'united', 'states', 'collision', 'forces', 'if', 'dictator', 'presidentdoes', 'amend', 'ways', 'myhusband', 'act', 'fast', 'deposited', 'funds', 'diplomaticcondominium', 'neighboringcountry', 'purpose', 'family', 'survival', 'war', 'might', 'beenoveri', 'writing', 'letter', 'dreams', 'fulfilled', 'couldnot', 'leave', 'iraq', 'thewar', 'however', 'succeeded', 'getting', 'children', 'neighboringcountry', 'first', 'tryingto', 'proceed', 'getting', 'out', 'it', 'process', 'us', 'my', 'husbandand', 'i', 'escaping', 'aneighboring', 'country', 'came', 'friendly', 'fire', 'it', 'mistake', 'thattook', 'husbandes', 'life', 'andcritically', 'injured', 'truck'])\n",
      " ...\n",
      " list(['jeff', '', 'thanks', 'phone', 'message', 'im', 'stuck', 'need', 'help', 'my', 'microsoft', 'word', 'working', 'i', 'want', 'send', 'something', 'ceos', 'asap', 'tuesday', 'can', 'clean', 'letter', 'im', 'fine', 'w', 'edits', 'for', 'signature', 'line', 'add', 'name', 'title', 'pls', 'send', 'final', 'letter', 'i', 'detach', 'ive', 'left', 'message', 'steve', 'wait', 'send', 'hear', 'from', 'him', 'im', 'collect', 'biz', 'cards', 'everyone', 'ceo', 'meetings', 'i', 'need', 'email', 'andor', 'fax', 'numbers', 'fortony', 'riddersteve', 'kirschken', 'oshmanfred', 'andersondan', 'scheinmanvivek', 'ranadivejohn', 'doerrray', 'iranican', 'send', 'info', 'cards', 'call', 'questions', 'thank', 'youkd'])\n",
      " list(['daren', '', 'also', 'deal', 'might', 'include', 'san', 'jacmeter', '', 'ena', 'kept', 'wholeal', '', 'forwarded', 'aimee', 'lannouhouect', '', '', 'am', '', 'aimee', 'lannou', '', '', 'amto', 'daren', 'j', 'farmerhouectectcc', 'subject', 'hlpdaren', '', 'pat', 'gave', 'spreadsheet', 'showing', 'allocated', 'greens', 'bayou', 'i', 'checked', 'deal', 'ticket', '', 'ends', '', 'there', 'is', 'still', 'two', 'days', 'need', 'reallocated', 'please', 'let', 'know', 'you', 'extend', 'deal', 'create', 'new', 'onethanksal'])\n",
      " list(['lee', 'johnson', 'would', 'like', 'come', 'tomorrow', 'friday', 'sign', 'facility', 'agreement', 'are', 'ready', 'go', 'he', 'really', 'wants', 'get', 'done', 'holidaysthankskay'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combined data is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data is:\")\n",
    "print(raw_data)\n",
    "\n",
    "# create corresponding labels\n",
    "Categories = ['spam','notspam']\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to convert these into numerical vectors!!\n",
    "\n",
    "**Featurize and Create Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.11758053e+00 -7.51736999e-01  5.53557634e-01 ... -6.60767317e-01\n",
      "   2.12305143e-01 -3.06020617e-01]\n",
      " [ 1.92008745e-02 -1.37052849e-01  1.80321768e-01 ...  5.39303459e-02\n",
      "  -6.54170811e-02  5.28319943e-05]\n",
      " [ 3.39616202e-02 -1.59206495e-01  7.21582845e-02 ...  1.01201266e-01\n",
      "  -1.98403805e-01 -6.55441135e-02]\n",
      " ...\n",
      " [-9.33180302e-02  7.55004538e-03  1.99574754e-01 ...  2.60701776e-01\n",
      "  -1.69033915e-01 -3.35301040e-04]\n",
      " [-3.09811551e-02 -2.08594531e-01  9.56941098e-02 ...  6.73941001e-02\n",
      "  -4.42153923e-02  5.69376582e-03]\n",
      " [-3.62663530e-02  8.49860311e-02  3.34817946e-01 ...  2.58871138e-01\n",
      "  -3.68074000e-01  3.20163995e-01]]\n"
     ]
    }
   ],
   "source": [
    "EmbeddingVectors = assemble_embedding_vectors(raw_data)\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x2/train_y2 (emails) list details, to make sure it is of the right form:\n",
      "1400\n",
      "[[ 0.03507754  0.06470443  0.07848432 ...  0.05978753 -0.23956941\n",
      "  -0.02912823]\n",
      " [-0.00287206 -0.0416818  -0.09273023 ...  0.15026653 -0.02471375\n",
      "   0.1012675 ]\n",
      " [-0.03710909 -0.21000084  0.2173935  ... -0.07776881 -0.15623932\n",
      "   0.21470238]\n",
      " ...\n",
      " [-0.0377222  -0.07692129 -0.0833061  ...  0.21315701 -0.12499469\n",
      "   0.07864657]\n",
      " [ 0.04721983 -0.05480211 -0.04854781 ... -0.04861398 -0.00052448\n",
      "   0.02917188]\n",
      " [-0.21808201 -0.10511982 -0.02095625 ... -0.2459277  -0.09837516\n",
      "  -0.01308274]]\n",
      "[1 1 0 1 1]\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# shuffle raw data first\n",
    "def unison_shuffle_data(data, header):\n",
    "    p = np.random.permutation(len(header))\n",
    "    data = data[p,:]\n",
    "    header = np.asarray(header)[p]\n",
    "    return data, header\n",
    "\n",
    "data, header = unison_shuffle_data(EmbeddingVectors, header)\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x2 = data[:idx,:]\n",
    "train_y2 = header[:idx]\n",
    "# # remaining 30% for testing\n",
    "test_x2 = data[idx:,:]\n",
    "test_y2 = header[idx:] \n",
    "\n",
    "print(\"train_x2/train_y2 (emails) list details, to make sure it is of the right form:\")\n",
    "print(len(train_x2))\n",
    "print(train_x2)\n",
    "print(train_y2[:5])\n",
    "print(len(train_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \"just email\" single-task shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 0s 301us/step - loss: 0.2111 - accuracy: 0.9457 - val_loss: 0.0763 - val_accuracy: 0.9850\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 0.0433 - accuracy: 0.9907 - val_loss: 0.1047 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 0.0317 - accuracy: 0.9921 - val_loss: 0.0656 - val_accuracy: 0.9850\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 122us/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 0.0769 - val_accuracy: 0.9833\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 122us/step - loss: 0.0136 - accuracy: 0.9971 - val_loss: 0.0757 - val_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 122us/step - loss: 0.0098 - accuracy: 0.9979 - val_loss: 0.0760 - val_accuracy: 0.9867\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 123us/step - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.0823 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 121us/step - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0880 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.0899 - val_accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.0929 - val_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors = Input(shape=input_shape)\n",
    "dense = Dense(512, activation='relu')(sent2vec_vectors)\n",
    "dense = Dropout(0.3)(dense)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=sent2vec_vectors, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_x2, train_y2, validation_data=(test_x2, test_y2), batch_size=32,\n",
    "                    nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Double-Task\" Email and IMDB System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "input1_shape = (len(train_x[0]),)\n",
    "input2_shape = (len(train_x2[0]),)\n",
    "sent2vec_vectors1 = Input(shape=input1_shape)\n",
    "sent2vec_vectors2 = Input(shape=input2_shape)\n",
    "combined = concatenate([sent2vec_vectors1,sent2vec_vectors2])\n",
    "dense1 = Dense(512, activation='relu')(combined)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "output1 = Dense(1, activation='sigmoid',name='classification1')(dense1)\n",
    "output2 = Dense(1, activation='sigmoid',name='classification2')(dense1)\n",
    "model = Model(inputs=[sent2vec_vectors1,sent2vec_vectors2], outputs=[output1,output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 1s 478us/step - loss: 0.9149 - classification1_loss: 0.6345 - classification2_loss: 0.2792 - classification1_accuracy: 0.6543 - classification2_accuracy: 0.9029 - val_loss: 0.6623 - val_classification1_loss: 0.5442 - val_classification2_loss: 0.1180 - val_classification1_accuracy: 0.7067 - val_classification2_accuracy: 0.9750\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 0s 190us/step - loss: 0.5272 - classification1_loss: 0.4595 - classification2_loss: 0.0672 - classification1_accuracy: 0.7964 - classification2_accuracy: 0.9864 - val_loss: 0.5254 - val_classification1_loss: 0.4457 - val_classification2_loss: 0.0788 - val_classification1_accuracy: 0.7883 - val_classification2_accuracy: 0.9783\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 0s 198us/step - loss: 0.4199 - classification1_loss: 0.3767 - classification2_loss: 0.0426 - classification1_accuracy: 0.8336 - classification2_accuracy: 0.9929 - val_loss: 0.5184 - val_classification1_loss: 0.4345 - val_classification2_loss: 0.0827 - val_classification1_accuracy: 0.8033 - val_classification2_accuracy: 0.9733\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 0s 191us/step - loss: 0.3640 - classification1_loss: 0.3328 - classification2_loss: 0.0303 - classification1_accuracy: 0.8557 - classification2_accuracy: 0.9929 - val_loss: 0.5079 - val_classification1_loss: 0.4341 - val_classification2_loss: 0.0732 - val_classification1_accuracy: 0.7817 - val_classification2_accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 0s 211us/step - loss: 0.3164 - classification1_loss: 0.2965 - classification2_loss: 0.0190 - classification1_accuracy: 0.8779 - classification2_accuracy: 0.9971 - val_loss: 0.5441 - val_classification1_loss: 0.4674 - val_classification2_loss: 0.0751 - val_classification1_accuracy: 0.7833 - val_classification2_accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 0s 189us/step - loss: 0.2814 - classification1_loss: 0.2667 - classification2_loss: 0.0152 - classification1_accuracy: 0.8886 - classification2_accuracy: 0.9979 - val_loss: 0.6158 - val_classification1_loss: 0.5388 - val_classification2_loss: 0.0743 - val_classification1_accuracy: 0.7700 - val_classification2_accuracy: 0.9800\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 0s 189us/step - loss: 0.2494 - classification1_loss: 0.2377 - classification2_loss: 0.0121 - classification1_accuracy: 0.9007 - classification2_accuracy: 0.9986 - val_loss: 0.5338 - val_classification1_loss: 0.4556 - val_classification2_loss: 0.0760 - val_classification1_accuracy: 0.7950 - val_classification2_accuracy: 0.9817\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 0s 198us/step - loss: 0.2449 - classification1_loss: 0.2369 - classification2_loss: 0.0079 - classification1_accuracy: 0.9107 - classification2_accuracy: 1.0000 - val_loss: 0.5614 - val_classification1_loss: 0.4800 - val_classification2_loss: 0.0787 - val_classification1_accuracy: 0.8000 - val_classification2_accuracy: 0.9800\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 0s 192us/step - loss: 0.2157 - classification1_loss: 0.2095 - classification2_loss: 0.0064 - classification1_accuracy: 0.9229 - classification2_accuracy: 1.0000 - val_loss: 0.5359 - val_classification1_loss: 0.4545 - val_classification2_loss: 0.0796 - val_classification1_accuracy: 0.7950 - val_classification2_accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 0s 194us/step - loss: 0.1947 - classification1_loss: 0.1888 - classification2_loss: 0.0056 - classification1_accuracy: 0.9221 - classification2_accuracy: 1.0000 - val_loss: 0.5407 - val_classification1_loss: 0.4574 - val_classification2_loss: 0.0810 - val_classification1_accuracy: 0.7967 - val_classification2_accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss={'classification1': 'binary_crossentropy', \n",
    "                    'classification2': 'binary_crossentropy'},\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit([train_x,train_x2],[train_y,train_y2],\n",
    "                    validation_data=([test_x,test_x2],[test_y,test_y2]),\n",
    "                                     batch_size=32, nb_epoch=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "#create_download_link(filename='file.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf aclImdb\n",
    "!rm aclImdb_v1.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
